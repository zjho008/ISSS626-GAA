---
title: "Hands-on Exercise 7: Geographically Weighted Regression (GWR)"
author: "Ho Zi Jun"
date: "Oct 07, 2024"
date-modified: "last-modified"
number-sections: true
execute:
  eval: true
  echo: true
  message: false
  freeze: true
editor: source
---

## Overview: Calibrating Hedonic Pricing Model for Private Highrise Properties with GWR Method

**Geographically weighted regression (GWR)** is a spatial statistical technique that takes non-stationary variables into consideration **(e.g., climate; demographic factors; physical environment characteristics)** and models the local relationships between these independent variables and as an outcome of interest (also known as dependent variable). In this hands-on exercise, we will learn how to build [hedonic pricing](https://www.investopedia.com/terms/h/hedonicpricing.asp) models by using GWR methods. The dependent variable is the **resale prices of condominium in 2015**. The independent variables are divided into either **structural** and/or **locational**.

## The Data

Two data sets will be used in this model building exercise, they are:

-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*) AND
-   condo_resale_2015 in csv format (i.e. *condo_resale_2015.csv*)

## Getting Started

Before getting started, it is important to install the necessary R packages into R and launch these R packages into the R environment.

The R packages needed for this exercise are as follows:

-   R package for building [Ordinary Least Squares regression (OLS)](https://link.springer.com/referenceworkentry/10.1007/978-94-007-0753-5_2008#:~:text=In%20its%20simplest%20form%2C%20OLS,change%20in%20x%2C%20and%20e) and performing diagnostics tests
    -   [**olsrr**](https://olsrr.rsquaredacademy.com/index.html)
-   R package for calibrating geographical weighted family of models
    -   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html)
-   R package for multivariate data visualisation and analysis
    -   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)
-   Spatial data handling
    -   **sf**
-   Attribute data handling
    -   **tidyverse**, especially **readr**, **ggplot2** and **dplyr**
-   Choropleth mapping
    -   **tmap**
-   Presentation-Ready Data Summary and Analytic Result Tables
    -   **gtsummary**

The code chunk below installs and launches these R packages into R environment.

```{r}
pacman::p_load(olsrr, GWmodel, corrplot, ggpubr, sf, spdep, tidyverse, tmap, gtsummary, broom.helpers)
```

## Short note about GWmodel

[**GWmodel**](https://www.jstatsoft.org/article/view/v063i17) package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. More commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.

In regression models - there is an intercept component where the line cuts through from an intercept.
E.g. when a floor unit area is zero - obviously no unit exists.

Beta(1) value can be +ve/-ve eg sq area vs age of property. 

## Geospatial Data Wrangling

### Importing geospatial data

The geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014's planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using `st_read()` of **sf** packages.

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                layer = "MP14_SUBZONE_WEB_PL")
```
::: callout-note
The result above shows that the R object used to contain the imported `MP14_SUBZONE_WEB_PL` shapefile is called *mpsz* and it is a simple feature object. The geometry type is *MULTIPOLYGON*. it is also important to note that the mpsz simple feature object **does not have** EPSG information.
:::

### Updating CRS information

The code chunk below updates the newly imported *mpsz* sf object with the correct ESPG code (i.e. 3414)

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

```{r}
# Check validity of geometries
sf::st_is_valid(mpsz_svy21)

# Attempt to fix invalid geometries
mpsz_svy21 <- sf::st_make_valid(mpsz_svy21)
```

After transforming the object, verification of the projection on the newly transformed *mpsz_svy21* is done by using `st_crs()` of **sf** package.

The code chunk below is used to verify the newly transformed *mpsz_svy21*.

```{r}
st_crs(mpsz_svy21)
```

Notice that the EPSG: is indicated as *3414* now. 

Next, the extent of *mpsz_svy21* is revealed by using `st_bbox()` of **sf** package.

```{r}
st_bbox(mpsz_svy21)
```
The extent of *mpsz_svy21* is illustrated from the results above.

## Aspatial Data Wrangling

### Importing the aspatial data

The *condo_resale_2015* is in csv file format. The codes chunk below uses `read_csv()` function of **readr** package to import *condo_resale_2015* into R as a tibble data frame called *condo_resale*.

```{r}
condo_resale <- read_csv("data/aspatial/Condo_resale_2015.csv")
```
After importing the aspatial data file into R, it is important to examine if the data file has been imported correctly.

The codes chunks below uses `glimpse()` and `head()` to display the data structure.

```{r}
glimpse(condo_resale)
```

```{r}
head(condo_resale$LONGITUDE) # to see the data in XCOORD column
head(condo_resale$LATITUDE) # to see the data in YCOORD column
```

Following which, `summary()` of base R is used to display the summary statistics of *cond_resale* tibble data frame.

```{r}
summary(condo_resale)
```

### Converting aspatial data frame into a sf object

The *condo_resale* tibble data frame is an aspatial data. We will convert it to a **sf** object. The code chunk below converts *condo_resale* data frame into a simple feature data frame by using `st_as_sf()` of **sf** packages.

```{r}
condo_resale.sf <- st_as_sf(condo_resale,
         coords = c("LONGITUDE", "LATITUDE"),
         crs = 4326) %>% 
  st_transform(crs = 3414)


condo_resale.sf
```

::: callout-note
Notice that `st_transform()` of **sf** package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).
:::

Next, `head()` is used to list the contents of *condo_resale.sf* object.

```{r}
head(condo_resale.sf)
```

::: callout-note
Notice that the output is in a point feature data frame.

> Geometry type: POINT
:::

## Exploratory Data Analysis (EDA)

In the section, we will learn how to use statistical graphic functions of **ggplot2** package to perform EDA.

### EDA using statistical graphics

We can plot the distribution of *AREA_SQM* by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.

```{r}
ggplot(data = condo_resale.sf,
       aes(x = `AREA_SQM`)) +
  geom_histogram(bins = 20,
                 color = "black",
                 fill = "light blue")
```

We can plot the distribution of *SELLING_PRICE* by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.

```{r}
ggplot(data = condo_resale.sf,
       aes(x = `SELLING_PRICE`)) +
  geom_histogram(bins = 20,
                 color = "black",
                 fill = "salmon")
```

The figure above reveals a right-skewed distribution. This means that more condominium units were transacted at relatively lower prices.

Statistically, the skewed distribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called *LOG_SELLING_PRICE* by using a *log transformation* on the variable *SELLING_PRICE*. It is performed using `mutate()` of **dplyr** package.

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

Plotting the *LOG_SELLING_PRICE* using the code chunk below.

```{r}
ggplot(data = condo_resale.sf,
       aes(x = `LOG_SELLING_PRICE`)) +
  geom_histogram(bins = 20,
                 color = "black",
                 fill = "salmon")
```

::: callout-note
## Note
The distribution is relatively less skewed after the transformation.
:::

### Multiple Histogram Plots distribution of variables

In this section, we will learn how to plot multiple small histograms (also known as a trellis plot) by using `ggarrange()` of [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/index.html) package.

The code chunk below is used to create 15 histograms. Then, `ggarrange()` is used to organised these histograms into a 3 columns by 5 rows small multiple plot.

```{r}
#| fig-width: 12
#| fig-height: 10
AREA_SQM <- ggplot(data = condo_resale.sf, aes(x = `AREA_SQM`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

AGE <- ggplot(data = condo_resale.sf, aes(x = `AGE`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_CBD <- ggplot(data = condo_resale.sf, aes(x = `PROX_CBD`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_CHILDCARE <- ggplot(data = condo_resale.sf, aes(x = `PROX_CHILDCARE`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_ELDERLYCARE <- ggplot(data = condo_resale.sf, aes(x = `PROX_ELDERLYCARE`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_URA_GROWTH_AREA <- ggplot(data = condo_resale.sf, aes(x = `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_HAWKER_MARKET <- ggplot(data = condo_resale.sf, aes(x = `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_KINDERGARTEN <- ggplot(data = condo_resale.sf, aes(x = `PROX_KINDERGARTEN`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_MRT <- ggplot(data = condo_resale.sf, aes(x = `PROX_MRT`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_PARK <- ggplot(data = condo_resale.sf, aes(x = `PROX_PARK`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_PRIMARY_SCH <- ggplot(data = condo_resale.sf, aes(x = `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_TOP_PRIMARY_SCH <- ggplot(data = condo_resale.sf, aes(x = `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_SHOPPING_MALL <- ggplot(data = condo_resale.sf, aes(x = `PROX_SHOPPING_MALL`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_SUPERMARKET <- ggplot(data = condo_resale.sf, aes(x = `PROX_SUPERMARKET`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

PROX_BUS_STOP <- ggplot(data = condo_resale.sf, aes(x = `PROX_BUS_STOP`)) +
  geom_histogram(bins = 20, color = "black", fill = "#56B4E9")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE,
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,
          PROX_MRT, PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,
          PROX_SHOPPING_MALL, PROX_SUPERMARKET, PROX_BUS_STOP,
          ncol = 3, nrow = 5)
```

### Drawing Statistical Point Map

Lastly, to reveal the geospatial distribution of condominium resale prices in Singapore. This will be done by plotting a map which will be prepared by using **tmap** package. Visually this can allow us to see a representation of which planning subzones might have higher or lower confominium resale prices.  

First, we will turn on the interactive mode of tmap by using the code chunk below.

```{r}
tmap_mode("view")
```

Next, the code chunk below is used to create an interactive point symbol map.

```{r}
# Set the tmap option to automatically check and fix invalid geometries
tmap_options(check.and.fix = TRUE)

tm_shape(mpsz_svy21) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.sf) +
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style = "quantile") +
  tm_view(set.zoom.limits = c(11,14))
```
::: callout-note
-   Notice that [`tm_dots()`](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_symbols) is used instead of `tm_bubbles()`.

-   `set.zoom.limits` argument of `tm_view()` sets the minimum and maximum zoom levels to 11 and 14 respectively.
:::

Before moving on to the next section, the code below will be used to turn R display into `plot` mode.

```{r}
tmap_mode("plot")
```

## Hedonic Pricing Modelling in R

In this section, it will illustrate how to build hedonic pricing models for condominium resale units using [`lm()`](https://www.rdocumentation.org/packages/stats/versions/3.5.2/topics/lm) of R base.

### Simple Linear Regression Method

First, we will build a simple linear regression model by using *SELLING_PRICE* as the dependent variable and *AREA_SQM* as the independent variable.

```{r}
condo.slr <- lm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

`lm()` returns an object of class "lm" or for multiple responses of class c("mlm", "lm").

The functions `summary()` and `anova()` can be used to obtain and print a summary and analysis of aa variance table of the results. The generic accessors functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by `lm`.

```{r}
summary(condo.slr)
```

The output report reveals that the *SELLING_PRICE* can be explained by using the formula:

```         
      *y = -258121.1 + 14719x1*
```

The **Multiple R-squared:  0.4518** reveals that the simple regression model built is able to explain about 45% of the resale prices.

Since p-value is much smaller than 0.0001 (p-value: < 2.2e-16
), we will reject the null hypothesis that  the mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of *SELLING_PRICE*.

The **Coefficients:** section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a result, we will be able to infer that the B0 and B1 are good parameter estimates.

To visualise the best fit curve on a scatter plot, we can incorporate `lm()` as a method function in ggplot's geometry as shown in the code chunk below.

```{r}
ggplot(data = condo_resale.sf,  
       aes(x =`AREA_SQM`, y =`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

the figure above reveals that there are a few statistical outliers with relatively high selling prices.

### Multiple Linear Regression Method

#### Visualising the relationships of the independent variables

Before building a multiple regression model, it is important to ensure that the independent variables used are not highly correlated to each other. If highly correlated independent variables are used in building a regression model, the quality of the model will be compromised. This phenomenon is known as **multicollinearity** in statistics.

Correlation matrix is commonly used to visualise the relationships between the independent variables. Besides the `pairs()` of R, there are many packages supporting the display of a correlation matrix. In this section, the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package will be used.

The code chunk below is used to plot a scatter plot matrix of the relationship between the independent variables in *condo_resale* data.frame.

```{r}
#| fig-width: 12
#| fig-height: 10
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

A matrix reorder is very important for mining the hidden structure and patterns in the matrix. There are four methods in corrplot (parameter order), named: "AOE", "FPC", "hclust", "alphabet". 

In the code chunk above, AOE order is used. It orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

From the scatterplot matrix, it is clear that ***Freehold*** is highly correlated to ***LEASE_99YEAR***. In view of this, it gives reason to include only either one of them in the subsequent model building. 

In this case, ***LEASE_99YEAR*** is excluded in the subsequent model building.

### Building a hedonic pricing model using multiple linear regression method

The code chunk below uses `lm()` to calibrate the multiple linear regression model.

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE	+ 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET	+ PROX_KINDERGARTEN	+ 
                  PROX_MRT	+ PROX_PARK	+ PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL	+ PROX_SUPERMARKET + 
                  PROX_BUS_STOP	+ NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                  data = condo_resale.sf)
summary(condo.mlr)
```

### Preparing Publication Quality Table: olsrr method

With reference to the report in the previous section, it is clear that not all the independent variables are statistically significant. The model can be revised  by removing those variables which are not statistically significant.

The removed variables are: *PROX_HAWKER_MARKET*, *PROX_KINDERGARTEN*,  *PROX_TOP_PRIMARY_SCH*, *PROX_SUPERMARKET*

The revised model is calibrated by using the code chunk below.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE	+
                   PROX_URA_GROWTH_AREA + PROX_MRT	+ PROX_PARK	+ 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL	+ PROX_BUS_STOP	+ 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

### Preparing Publication Quality Table: gtsummary method

The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/index.html) package provides an elegant and flexible way to create publication-ready summary tables in R.

The code chunk below using [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report.

```{r}
tbl_regression(condo.mlr1, intercept = TRUE)
```

With gtsummary package, model statistics can be included in the report by either appending them to the report table by using [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) or adding the model statistics as a table source note by using [`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) as shown in the code chunk below.

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

> For more customisation options, refer to [Tutorial: tbl_regression](https://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)

#### Checking for multicolinearity

In this section, it will introduce a R package specially programmed for performing OLS regression. It is called [**olsrr**](https://olsrr.rsquaredacademy.com/). It provides a collection of very useful methods for building better multiple linear regression models:

-   comprehensive regression output
-   residual diagnostics
-   measures of influence
-   heteroskedasticity tests
-   collinearity diagnostics
-   model fit assessment
-   variable contribution assessment
-   variable selection procedures

In the code chunk below, the [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) function of **olsrr** package is used to test if there are signs of multicollinearity.

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIF of the independent variables are all less than 10. We can safely conclude that there are no signs of multicollinearity among the independent variables.

#### Test for Non-Linearity

In multiple linear regression, it is important to test the assumption of linearity and additivity of the relationship between dependent and independent variables.

In the code chunk below, the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) of **olsrr** package is used to perform a linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

The figure above reveals that most of the data points are scattered around the 0 line, hence it is safe to conclude that the relationships between the dependent variable and independent variables are linear.

#### Test for Normality Assumption

Lastly, the code chunk below uses [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of *olsrr* package to perform a normality assumption test.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

The figure above reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resembling a normal distribution.

For formal statistical test methods, the [`ols_test_normality()`](https://olsrr.rsquaredacademy.com/reference/ols_test_normality.html) of **olsrr** package can be used as shown in the code chunk below.

```{r}
ols_test_normality(condo.mlr1)
```

The summary table above reveals that the p-values for all four tests are smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.

#### Testing for Spatial Autocorrelation

The hedonic model to be built is done by using geographically referenced attributes, hence it is also important for us to visualise the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert *condo_resale.sf* from sf data frame into a **SpatialPointsDataFrame**.

First, we will export the residual of the hedonic pricing model and save it as a data frame.

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

Next, to join the newly created data frame with *condo_resale.sf* object.

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

Next, to convert *condo_resale.res.sf* from simple feature object into a SpatialPointsDataFrame because **spdep** package can only process sp conformed spatial data objects.

The code chunk below will be used to perform the data conversion process.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

Folowing which, the **tmap** package is used to display the distribution of the residuals on an interactive map.

The code churn below will turn on the interactive mode of tmap.

```{r}
tmap_mode("view")
```

The code chunks below is used to create an interactive point symbol map.

```{r}
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style ="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

> Note to switch back to "plot" mode before continuing.

```{r}
tmap_mode("plot")
```

The figure above revealed that there sre signs  of spatial autocorrelation.

To proof that this observation is indeed true, the Moran's I test will be performed

The distance-based weight matrix will be computed by using [`dnearneigh()`](https://r-spatial.github.io/spdep/reference/dnearneigh.html) function of **spdep**.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

Subsequently, [`nb2listw()`](https://r-spatial.github.io/spdep/reference/nb2listw.html) of **spdep** package will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

The [`lm.morantest()`](https://r-spatial.github.io/spdep/reference/lm.morantest.html) of **spdep** package will be used to perform Moran's I test for residual spatial autocorrelation

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we can conclude to reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran I = 0.1438876 which is greater than 0, we can infer that the residuals resemble cluster distribution.

## Building Hedonic Pricing Models using GWmodel

In this section, it illustrates how to model a hedonic pricing using both the fixed and adaptive bandwidth schemes

### Building Fixed Bandwidth GWR Model

#### Computing fixed bandwith

In the code chunk below `bw.gwr()` of GWModel package is used to determine the optimal fixed bandwidth to use in the model. 

::: callout-note
Notice that the argument ***adaptive*** is set to **FALSE** indicates that we are interested to compute the fixed bandwidth.
:::

There are two possible approaches can be used to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule  by using the ***approach*** agreement.

```{r}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE	+ PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE	+ PROX_URA_GROWTH_AREA + 
                     PROX_MRT	+ PROX_PARK	+ PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL	+ PROX_BUS_STOP	+ NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data = condo_resale.sp, 
                   approach = "CV", 
                   kernel = "gaussian", 
                   adaptive = FALSE, 
                   longlat = FALSE)
```

The result shows that the recommended bandwidth is 971.3405 metres. 

> Question  - Why is the bandwidth shown in metres? 

#### GWModel method - fixed bandwith

Now we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.

```{r}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE	+ PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE	+ PROX_URA_GROWTH_AREA + 
                         PROX_MRT	+ PROX_PARK	+ PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL	+ PROX_BUS_STOP	+ NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data = condo_resale.sp, 
                       bw = bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

The output is saved in a list of class "gwrm". The code below can be used to display the model output.

```{r}
gwr.fixed
```

The report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the global multiple linear regression model of 42967.1.

### Building Adaptive Bandwidth GWR Model

In this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.

#### Computing the adaptive bandwidth

Similar to the earlier section, we will first use `bw.gwr()` to determine the recommended data point to use.

The code chunk used look very similar to the one used to compute the fixed bandwidth except the `adaptive` argument has changed to **TRUE**.

```{r}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE	+ 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE	+ 
                        PROX_URA_GROWTH_AREA + PROX_MRT	+ PROX_PARK	+ 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL	+ PROX_BUS_STOP	+ 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data = condo_resale.sp, 
                      approach = "CV", 
                      kernel = "gaussian", 
                      adaptive = TRUE, 
                      longlat = FALSE)
```

The result shows that the 30 is the recommended number of data points to be used.

#### Constructing the adaptive bandwidth gwr model

Now, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.

```{r}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT	+ PROX_PARK	+ 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data = condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

The code below can be used to display the model output.

```{r}
gwr.adaptive
```

The report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.

### Visualising GWR Output

In addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:

-   Condition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.

-   Local R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.

-   Predicted: these are the estimated (or fitted) y values 3. computed by GWR.

-   Residuals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.

-   Coefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.

They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its "data" slot in an object called **SDF** of the output list.

### Converting SDF into *sf* data.frame

To visualise the fields in **SDF**, we need to first covert it into **sf** data.frame by using the code chunk below.

```{r}
condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
```

```{r}
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)
condo_resale.sf.adaptive.svy21  
```

```{r}
gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))
```

Next, `glimpse()` is used to display the content of *condo_resale.sf.adaptive* sf data frame.

```{r}
glimpse(condo_resale.sf.adaptive)
```

```{r}
summary(gwr.adaptive$SDF$yhat)
```

### Visualising local R2

The code chunk below is used to create an interactive point symbol map.

```{r}
tmap_mode("view")
tm_shape(mpsz_svy21) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray50",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```

```{r echo=TRUE, eval=TRUE}
tmap_mode("plot")
```

### Visualising coefficient estimates

The code chunks below are used to create an interactive point symbol map.

```{r}
tmap_mode("view")
AREA_SQM_SE <- tm_shape(mpsz_svy21) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

AREA_SQM_TV <- tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
             asp=1, ncol=2,
             sync = TRUE)
```

```{r echo=TRUE, eval=TRUE}
tmap_mode("plot")
```

#### By URA Plannign Region

```{r}
#| fig-height: 6
#| fig-width: 6
#| fig-align: center

tmap_options(check.and.fix = TRUE) +

tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ]) +
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)
```

## References

-   Gollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) "GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models". *Journal of Statistical Software*, 63(17):1-50, http://www.jstatsoft.org/v63/i17/

-   Lu B, Harris P, Charlton M, Brunsdon C (2014) "The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models". *Geo-spatial Information Science* 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453


***END***
