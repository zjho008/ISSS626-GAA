---
title: "Hands-on Exercise 8: Geographically Weighted Predictive Modelling"
author: "Ho Zi Jun"
date: "Oct 17, 2024"
date-modified: "last-modified"
number-sections: true
execute:
  eval: true
  echo: true
  message: false
  freeze: true
editor: source
---

## Overview: Geographically Weighted Predictive Models

Predictive modelling employs statistical learning or machine learning techniques to forecast outcomes, often targeting future events. These models are calibrated using a set of known outcomes and predictors (or variables).

Geospatial predictive modelling is based on the idea that the events being predicted are spatially constrained, meaning their distribution is not random or uniform across space. When working with geographically referenced data, factors such as infrastructure, sociocultural elements, and topography influence where events occur. Geospatial predictive modelling aims to explain these spatial constraints and influences by correlating past event locations with environmental factors that shape their distribution.

### Learning Outcomes

In this hands-on exercise, the goal is to learn how to build predictive models using geographical random forest method. By the end of this hands-on exercise, it would have helped to acquire the skills of:

-   preparing training and test data sets by using appropriate data sampling methods,
-   calibrating predictive models by using both geospatial statistical learning and machine learning methods,
-   comparing and selecting the best model for predicting the future outcome,
-   predicting the future outcomes by using the best model calibrated.

## The Data

-   **Aspatial dataset**:
    -   HDB Resale data: a list of HDB resale transacted prices in Singapore from Jan 2017 onwards. It is in csv format which can be downloaded from Data.gov.sg.
-   **Geospatial dataset**:
    -   *MP14_SUBZONE_WEB_PL*: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg
-   **Locational factors with geographic coordinates**:
    -   Downloaded from **Data.gov.sg**.
        -   **Eldercare** data is a list of eldercares in Singapore. It is in shapefile format.
        -   **Hawker Centre** data is a list of hawker centres in Singapore. It is in geojson format.
        -   **Parks** data is a list of parks in Singapore. It is in geojson format.
        -   **Supermarket** data is a list of supermarkets in Singapore. It is in geojson format.
        -   **CHAS clinics** data is a list of CHAS clinics in Singapore. It is in geojson format.
        -   **Childcare service** data is a list of childcare services in Singapore. It is in geojson format.
        -   **Kindergartens** data is a list of kindergartens in Singapore. It is in geojson format.
    -   Downloaded from **Datamall.lta.gov.sg**.
        -   **MRT** data is a list of MRT/LRT stations in Singapore with the station names and codes. It is in shapefile format.
        -   **Bus stops** data is a list of bus stops in Singapore. It is in shapefile format.
-   **Locational factors without geographic coordinates**:
    -   Downloaded from **Data.gov.sg**.
        -   **Primary school** data is extracted from the list on General information of schools from data.gov portal. It is in csv format.
    -   Retrieved/Scraped from **other sources**
        -   **CBD** coordinates obtained from Google.
        -   **Shopping malls** data is a list of Shopping malls in Singapore obtained from [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore).
        -   **Good primary schools** is a list of primary schools that are ordered in ranking in terms of popularity and this can be found at [Local Salary Forum](https://www.salary.sg/2021/best-primary-schools-2021-by-popularity).

## Installing and Loading R Packages

The code chunk below performs 3 tasks:

-   A list of all the R packages required to accomplish this exercise will be called.
-   Check if R packages or package have been installed in R, otherwise they will be installed.
-   After all the R packages have been installed, they will be loaded into the R environment.

```{r}
pacman::p_load(sf, spdep, GWmodel, SpatialML,
               tmap, rsample, Metrics, tidyverse)
```

## Preparing Data

### Reading data file to rds

The code chunk below is used to read the input data sets. It is in simple feature data frame.

```{r}
mdata <- read_rds("data/mdata.rds")
```

### Data Sampling

The dataset is divided into training (65%) and testing (35%) subsets using the `initial_split()` function from the **rsample** package, part of the [tidymodels](https://www.tidymodels.org/) suite in R. This split supports model training and evaluation while adhering to best practices for data analysis.

```{r}
set.seed(1234)
resale_split <- initial_split(mdata,
                              prop = 6.5/10,)
train_data <- training(resale_split)
test_data <- testing(resale_split)
```

The code chunk below is used to save the training and test data into rds format for ease of retrieval.

```{r}
write_rds(train_data, "data/model/train_data.rds")
write_rds(test_data, "data/model/test_data.rds")
```

## Computing Correlation Matrix

Before loading predictors into a predictive model, it’s best practice to check for multicollinearity using a correlation matrix. This helps identify highly correlated predictors that could distort model accuracy.

```{r}
#| fig-width: 10
#| fig-height: 12
mdata_nogeo <- mdata %>%
  st_drop_geometry()
corrplot::corrplot(cor(mdata_nogeo[, 2:17]),
                   diag = FALSE,
                   order = "AOE",
                   tl.pos = "td",
                   tl.cex = 0.5,
                   method = "number",
                   type = "upper")
```

::: callout-note
The correlation matrix above shows that all the correlation values are below 0.8. Hence, there is no sign of multicollinearity observed.
:::

## Retriving the Stored Data

The training and testing datasets that were stored earlier are now retrieved.

```{r}
train_data <- read_rds("data/model/train_data.rds")
test_data <- read_rds("data/model/test_data.rds")
```

## Building a non-spatial multiple linear regression

```{r}
price_mlr <- lm(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data = train_data)
summary(price_mlr)
```

The non-spatial multiple linear regression is then saved in rds file format for purposes of easy retrival when needed.

```{r}
write_rds(price_mlr, "data/model/price_mlr.rds" ) 
```

## gwr predictive method

In this section, we will learn how to calibrate a model to predict HDB resale prices by using the geographically weighted regression method of [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html) package.

### Converting the training data from sf data.frame to SpatialPointDataFrame

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

### Computing adaptive bandwidth for the training data

Next, `bw.gwr()` of **GWmodel** package will be used to determine the optimal bandwidth to be used.

::: callout-note
The code chunk below is used to determine the adaptive bandwidth. Additionally, CV method is used to determine the optimal bandwidth.
:::

```{r}
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data = train_data_sp,
                  approach = "CV",
                  kernel = "gaussian",
                  adaptive = TRUE,
                  longlat = FALSE)
```

The result above shows that 40 neighbour points will be the optimal bandwidth to be used if adaptive bandwidth is used for this data set.

The result is then saved as rds format as well.

```{r}
write_rds(bw_adaptive, "data/model/bw_adaptive.rds")
```

### Constructing the adaptive bandwidth gwr model

The code chunk below will call the saved bandwidth by using the code chunk below.

```{r}
bw_adaptive <- read_rds("data/model/bw_adaptive.rds")
```

The code chunk below is then used to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.

::: callout-note
The use of the same *kernel* and *adaptive* arguments in both `bw.gwr()` and `gwr.basic()` maintains consistency in the model’s weighting and spatial adaptiveness. The `gaussian` kernel ensures smooth distance-based weighting, while `adaptive = TRUE` adjusts the bandwidth to account for the varying density of spatial observations. This setup allows for a more accurate fit in geographically weighted regression (GWR), especially in heterogeneous spatial data, as it adapts locally to data point distribution.
:::

```{r}
gwr_adaptive <- gwr.basic(formula = resale_price ~
                            floor_area_sqm + storey_order +
                            remaining_lease_mths + PROX_CBD + 
                            PROX_ELDERLYCARE + PROX_HAWKER +
                            PROX_MRT + PROX_PARK + PROX_MALL + 
                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                            WITHIN_1KM_PRISCH,
                          data = train_data_sp,
                          bw = bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive = TRUE,
                          longlat = FALSE)
```

The code chunk below will be used to save the model in rds format for future use similar to the steps above to prevent having to re-run the code chunk above which can be time consuming.

```{r}
write_rds(gwr_adaptive, "data/model/gwr_adaptive.rds")
```

### Retrieving gwr output object

The code chunk below will be used to retrieve the saved gwr model object in the previous section.

```{r}
gwr_adaptive <- read_rds("data/model/gwr_adaptive.rds")
```

The code chunk below is used to display the model output.

```{r}
gwr_adaptive
```

### Converting the test data from sf data.frame to SpatialPointDataFrame

```{r}
test_data_sp <- test_data %>%
  as_Spatial()
test_data_sp
```

### Computing adaptive bandwidth for the test data

```{r}
bw_test_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data = test_data_sp,
                  approach = "CV",
                  kernel = "gaussian",
                  adaptive = TRUE,
                  longlat = FALSE)
```

The results above indicates that an adaptive bandwidth of **25 nearest neighbors** yields the lowest cross-validation (CV) score of (4.04299e+12), making it the optimal bandwidth for this test dataset when using adaptive bandwidth selection. This value suggests that 25 nearby data points provide the best balance between model fit and generalization for the geographic weighting applied in this test data’s GWR model.

The result is then saved as rds format as well.

```{r}
write_rds(bw_test_adaptive, "data/model/bw_test_adaptive.rds")
```

The code chunk below will call the saved bandwidth by using the code chunk below.

```{r}
bw_test_adaptive <- read_rds("data/model/bw_test_adaptive.rds")
```

### Computing predicted values of the test data

```{r}
#| eval: false
gwr_pred <- gwr.predict(formula = resale_price ~
                          floor_area_sqm + storey_order +
                          remaining_lease_mths + PROX_CBD + 
                          PROX_ELDERLYCARE + PROX_HAWKER + 
                          PROX_MRT + PROX_PARK + PROX_MALL + 
                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
                          WITHIN_1KM_PRISCH, 
                        data = train_data_sp, 
                        predictdata = test_data_sp, 
                        bw = 40, 
                        kernel = 'gaussian', 
                        adaptive = TRUE, 
                        longlat = FALSE)
```

## Preparing coordinates data

### Extracting coordinates data

The code chunk below extracts the x,y coordinates from the full, training and test data sets respectively.

```{r}
coords <- st_coordinates(mdata)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

Before moving on, we will write all the output into rds for future usage.

```{r}
coords_train <- write_rds(coords_train, "data/model/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/model/coords_test.rds" )
```

### Dropping geometry field

We will drop the geometry column of the sf data.frame by using `st_drop_geometry()` of sf package.

```{r}
train_data <- train_data %>% 
  st_drop_geometry()
```

## Calibrating Random Forest Model

In this section, it will calibrate a model to predict HDB resale price by using random forest function of [**ranger**](https://cran.r-project.org/web/packages/ranger/index.html) package.

```{r}
set.seed(1234)

rf <- ranger(resale_price ~ floor_area_sqm + storey_order + 
               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + 
               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + 
               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
               WITHIN_1KM_PRISCH,
             data = train_data)
rf
```

The output is saved into rds for future usage.

```{r}
write_rds(rf, "data/model/rf.rds")
```

```{r}
rf <- read_rds("data/model/rf.rds")
rf
```

## Calibrating Geographical Random Forest Model

In this section, the steps involved illustrate how to calibrate a model to predict HDB resale price by using `grf()` of [**SpatialML**](https://cran.r-project.org/web/packages/ranger/index.html) package.

### Calibrating using training data

The code chunk below calibrates a geographic random forest model by using `grf()` of **SpatialML** package.

```{r}
set.seed(1234)

gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order +
                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +
                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +
                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                       WITHIN_1KM_PRISCH,
                     dframe = train_data, 
                     bw = 55,
                     kernel = "adaptive",
                     coords = coords_train)
```

Saving the model output by using the code chunk below.

```{r}
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

The code chunk below can then be used to retrieve the saved model in future.

```{r}
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

### Predicting by using test data

#### Preparing the test data

The code chunk below is used to combine the test data with its corresponding coordinates data; alongside dropping of the geometry.

```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

#### Predicting with test data

Next, `predict.grf()` of spatialML package will be used to predict the resale value of HDB flats by using the `test data` and `gwRF_adaptive` model calibrated earlier.

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name = "X",
                           y.var.name = "Y", 
                           local.w = 1,
                           global.w = 0)
```

The output is saved into rds file for future use.

```{r}
GRF_pred <- write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

#### Converting the predicting output into a data frame

The output of the `predict.grf()` is a vector of predicted values. It is advisable to convert it into a data frame for further visualisation and analysis.

The output is first saved as an rds file before being converted into a data frame from the code chunk below.

```{r}
GRF_pred <- read_rds("data/model/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

In the code chunk below, `cbind()` is used to append the predicted values onto the test_data.

```{r}
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r}
write_rds(test_data_p, "data/model/test_data_p.rds")
```

### Calculating Root Mean Square Error

The calculation of root mean square error (RMSE) will allow us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, `rmse()` of Metrics package is used to compute the RMSE.

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

### Visualising the predicted values

Alternatively, a scatter plot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

::: callout-note
A better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model.
:::

